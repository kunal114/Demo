import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

class RuleValidator:
    """
    Validates discovered transformation rules by applying them to test data
    and comparing the results with actual output values.
    """
    
    def validate_rules(self, all_rules, input_df, output_df, categorical_handler, test_size=0.2, random_state=42):
        """
        Validate discovered rules on test data.
        
        Args:
            all_rules (dict): Dictionary of discovered rules by output column and segment
            input_df (pd.DataFrame): Input dataset
            output_df (pd.DataFrame): Output dataset
            categorical_handler (CategoricalHandler): Handler for categorical data
            test_size (float): Proportion of data to use for testing
            random_state (int): Random seed for reproducibility
            
        Returns:
            float: Overall validation accuracy
        """
        # Split data into train and test sets
        input_train, input_test, output_train, output_test = train_test_split(
            input_df, output_df, test_size=test_size, random_state=random_state
        )
        
        # Apply rules to test data
        predicted_output = self._apply_rules(all_rules, input_test, categorical_handler)
        
        # Calculate accuracy per output column
        column_accuracies = {}
        for col in output_test.columns:
            if col in predicted_output.columns:
                # Calculate normalized RMSE
                rmse = np.sqrt(np.mean((predicted_output[col] - output_test[col]) ** 2))
                
                # Normalize by the range of the output
                output_range = output_test[col].max() - output_test[col].min()
                if output_range > 0:
                    normalized_rmse = rmse / output_range
                    accuracy = max(0, 1 - normalized_rmse)
                else:
                    # If range is 0, check if predictions match exactly
                    accuracy = float(np.array_equal(predicted_output[col], output_test[col]))
                
                column_accuracies[col] = accuracy
        
        # Calculate overall accuracy
        if column_accuracies:
            overall_accuracy = sum(column_accuracies.values()) / len(column_accuracies)
        else:
            overall_accuracy = 0.0
            
        # Print detailed accuracy report
        print("\nValidation Results:")
        print(f"{'Column':<15} {'Accuracy':<10}")
        print("-" * 25)
        for col, acc in column_accuracies.items():
            print(f"{col:<15} {acc:.4f}")
            
        return overall_accuracy
    
    def _apply_rules(self, all_rules, input_df, categorical_handler):
        """
        Apply discovered rules to input data to predict outputs.
        
        Args:
            all_rules (dict): Dictionary of discovered rules by output column and segment
            input_df (pd.DataFrame): Input dataset
            categorical_handler (CategoricalHandler): Handler for categorical data
            
        Returns:
            pd.DataFrame: Predicted output dataset
        """
        # Initialize empty DataFrame for predictions
        predicted_output = pd.DataFrame(index=input_df.index)
        
        # Segment the input data
        segments = categorical_handler.segment_data(input_df)
        
        # Apply rules to each segment
        for output_col, rules_by_segment in all_rules.items():
            # Initialize column with NaN values
            predicted_output[output_col] = np.nan
            
            # Apply segment-specific rules
            for segment_name, rule in rules_by_segment.items():
                if segment_name in segments:
                    segment_df = segments[segment_name]
                    segment_indices = segment_df.index
                    
                    try:
                        predicted_values = self._evaluate_rule(rule, segment_df)
                        predicted_output.loc[segment_indices, output_col] = predicted_values
                    except Exception as e:
                        print(f"Error applying rule '{rule}' for {output_col} in segment {segment_name}: {e}")
        
        return predicted_output
    
    def _evaluate_rule(self, rule, input_df):
        """
        Evaluate a rule string on the input data.
        
        Args:
            rule (str): Rule string to evaluate
            input_df (pd.DataFrame): Input dataframe
            
        Returns:
            np.ndarray: Predicted values
        """
        # Handle different rule types
        if "Unknown transformation" in rule:
            # Return NaN for unknown rules
            return np.full(len(input_df), np.nan)
            
        if "Decision tree" in rule:
            # For decision tree rules, we can't directly evaluate
            # In a real implementation, you would store the trained model
            return np.full(len(input_df), np.nan)
        
        # For simple arithmetic rules
        try:
            # Replace column names with actual values
            numeric_input_df = input_df.copy()
            if isinstance(numeric_input_df, pd.Series):
                numeric_input_df = pd.DataFrame(numeric_input_df)
            
            # Create a dictionary mapping column names to their values
            columns_dict = {}
            for col in numeric_input_df.columns:
                if col in rule:
                    columns_dict[col] = numeric_input_df[col].values
            
            # Simple parsing and evaluation for basic arithmetic
            # This is a simplified implementation - a real one would use a proper expression parser
            result = self._simple_rule_evaluation(rule, columns_dict)
            return result
            
        except Exception as e:
            print(f"Error evaluating rule '{rule}': {e}")
            return np.full(len(input_df), np.nan)
    
    def _simple_rule_evaluation(self, rule, columns_dict):
        """
        Simple evaluation of arithmetic rules.
        
        Args:
            rule (str): Rule string to evaluate
            columns_dict (dict): Dictionary mapping column names to their values
            
        Returns:
            np.ndarray: Result of rule evaluation
        """
        # This is a simplified implementation - in practice, you'd use a proper expression parser
        # Handle some common cases
        
        # Direct column reference
        if rule in columns_dict:
            return columns_dict[rule]
        
        # Constant multiplication: "2 * col"
        if '*' in rule and len(rule.split('*')) == 2:
            parts = rule.split('*')
            factor_part = parts[0].strip()
            col_part = parts[1].strip()
            
            if col_part in columns_dict:
                try:
                    factor = float(factor_part)
                    return factor * columns_dict[col_part]
                except:
                    pass
            
            if factor_part in columns_dict:
                try:
                    factor = float(col_part)
                    return factor * columns_dict[factor_part]
                except:
                    pass
                    
            if factor_part in columns_dict and col_part in columns_dict:
                return columns_dict[factor_part] * columns_dict[col_part]
        
        # Addition: "col1 + col2" or "col + constant"
        if '+' in rule and len(rule.split('+')) == 2:
            parts = rule.split('+')
            left_part = parts[0].strip()
            right_part = parts[1].strip()
            
            # col + constant
            if left_part in columns_dict:
                try:
                    constant = float(right_part)
                    return columns_dict[left_part] + constant
                except:
                    pass
            
            # constant + col
            if right_part in columns_dict:
                try:
                    constant = float(left_part)
                    return constant + columns_dict[right_part]
                except:
                    pass
            
            # col1 + col2
            if left_part in columns_dict and right_part in columns_dict:
                return columns_dict[left_part] + columns_dict[right_part]
        
        # Subtraction: "col1 - col2" or "col - constant"
        if '-' in rule and len(rule.split('-')) == 2:
            parts = rule.split('-')
            left_part = parts[0].strip()
            right_part = parts[1].strip()
            
            # col - constant
            if left_part in columns_dict:
                try:
                    constant = float(right_part)
                    return columns_dict[left_part] - constant
                except:
                    pass
            
            # constant - col (less common)
            if right_part in columns_dict:
                try:
                    constant = float(left_part)
                    return constant - columns_dict[right_part]
                except:
                    pass
            
            # col1 - col2
            if left_part in columns_dict and right_part in columns_dict:
                return columns_dict[left_part] - columns_dict[right_part]
        
        # Division: "col1 / col2" or "col / constant"
        if '/' in rule and len(rule.split('/')) == 2:
            parts = rule.split('/')
            left_part = parts[0].strip()
            right_part = parts[1].strip()
            
            # col / constant
            if left_part in columns_dict:
                try:
                    constant = float(right_part)
                    return np.divide(columns_dict[left_part], constant)
                except:
                    pass
            
            # constant / col
            if right_part in columns_dict:
                try:
                    constant = float(left_part)
                    return np.divide(constant, columns_dict[right_part], 
                                    out=np.zeros_like(columns_dict[right_part]), 
                                    where=columns_dict[right_part]!=0)
                except:
                    pass
            
            # col1 / col2
            if left_part in columns_dict and right_part in columns_dict:
                return np.divide(columns_dict[left_part], columns_dict[right_part],
                                out=np.zeros_like(columns_dict[left_part]),
                                where=columns_dict[right_part]!=0)
        
        # Square: "col^2"
        if '^2' in rule:
            col = rule.replace('^2', '').strip()
            if col in columns_dict:
                return columns_dict[col] ** 2
        
        # Square root: "sqrt(col)"
        if rule.startswith('sqrt(') and rule.endswith(')'):
            col = rule[5:-1].strip()
            if col in columns_dict:
                return np.sqrt(np.maximum(columns_dict[col], 0))  # Avoid negative values
        
        # Log: "log(col)"
        if rule.startswith('log(') and rule.endswith(')'):
            col = rule[4:-1].strip()
            if col in columns_dict:
                return np.log(np.maximum(columns_dict[col], 1e-10))  # Avoid log(0)
        
        # Exp: "exp(col)"
        if rule.startswith('exp(') and rule.endswith(')'):
            col = rule[4:-1].strip()
            if col in columns_dict:
                return np.exp(np.minimum(columns_dict[col], 20))  # Avoid overflow
        
        # If we can't parse it, return NaN
        return np.full(len(next(iter(columns_dict.values()))), np.nan)







import pandas as pd
import numpy as np
from dataset_generator import DatasetGenerator
from rule_discoverer import RuleDiscoverer
from rule_validator import RuleValidator
from categorical_handler import CategoricalHandler
from feature_engineering import FeatureEngineer
import json

def run_example():
    """Run an example of the automatic rule discovery system"""
    print("=== Automatic Rule Discovery System ===\n")
    
    # 1. Generate synthetic data with known rules
    print("Generating synthetic data...")
    generator = DatasetGenerator(n_samples=200, n_input_cols=4, n_output_cols=6, categorical_cols=1)
    input_df, output_df, true_rules = generator.generate_datasets()
    
    print(f"Generated input dataset with {len(input_df.columns)} columns and {len(input_df)} rows")
    print(f"Generated output dataset with {len(output_df.columns)} columns and {len(output_df)} rows")
    
    # Print sample data
    print("\nSample input data:")
    print(input_df.head(3))
    print("\nSample output data:")
    print(output_df.head(3))
    
    # Print true rules
    generator.print_true_rules(true_rules)
    
    # 2. Discover rules
    print("Starting rule discovery process...")
    categorical_cols = [col for col in input_df.columns if col.startswith('category')]
    
    # Initialize components
    categorical_handler = CategoricalHandler(categorical_cols)
    feature_engineer = FeatureEngineer(input_df, output_df)
    rule_discoverer = RuleDiscoverer(categorical_handler)
    rule_validator = RuleValidator()
    
    # Check if we need feature engineering (M > N)
    if len(output_df.columns) > len(input_df.columns):
        print(f"M ({len(output_df.columns)}) > N ({len(input_df.columns)}): Performing feature generation...")
        enhanced_input_df = feature_engineer.generate_candidate_features()
    else:
        enhanced_input_df = input_df.copy()
    
    # Segment data by categorical features
    segments = categorical_handler.segment_data(enhanced_input_df)
    print(f"Data segmented into {len(segments)} segments based on categorical features")
    
    # Discover rules per segment
    all_rules = {}
    for segment_name, segment_df in segments.items():
        segment_output = categorical_handler.get_corresponding_output(output_df, segment_df)
        
        print(f"Discovering rules for segment: {segment_name}")
        segment_rules = rule_discoverer.discover_rules(segment_df, segment_output)
        
        # Store discovered rules with segment information
        for output_col, rule in segment_rules.items():
            if output_col not in all_rules:
                all_rules[output_col] = {}
            all_rules[output_col][segment_name] = rule
    
    # 3. Validate rules
    print("\nValidating discovered rules...")
    validation_accuracy = rule_validator.validate_rules(all_rules, enhanced_input_df, output_df, categorical_handler)
    
    print(f"\nOverall rule validation accuracy: {validation_accuracy:.2f}")
    
    # 4. Compare discovered rules with true rules
    print("\n=== DISCOVERED TRANSFORMATION RULES ===")
    for output_col, rules_by_segment in all_rules.items():
        print(f"\n{output_col}:")
        if len(rules_by_segment) == 1:
            # Single rule for all data
            print(f"  {next(iter(rules_by_segment.values()))}")
        else:
            # Conditional rules
            for segment, rule in rules_by_segment.items():
                print(f"  When {segment}: {rule}")
    print("======================================\n")
    
    # 5. Export discovered rules to JSON
    with open('discovered_rules.json', 'w') as f:
        # Convert numpy types to Python native types for JSON serialization
        json_rules = {}
        for output_col, rules_by_segment in all_rules.items():
            json_rules[output_col] = {segment: rule for segment, rule in rules_by_segment.items()}
        
        json.dump(json_rules, f, indent=2)
    
    print("Rules exported to 'discovered_rules.json'")

def run_real_data_example(input_file, output_file, categorical_cols=None):
    """
    Run the rule discovery system on real data from CSV files.
    
    Args:
        input_file (str): Path to input CSV file
        output_file (str): Path to output CSV file
        categorical_cols (list): List of categorical columns in the input data
    """
    print("=== Automatic Rule Discovery on Real Data ===\n")
    
    # 1. Load data
    print(f"Loading data from {input_file} and {output_file}...")
    input_df = pd.read_csv(input_file)
    output_df = pd.read_csv(output_file)
    
    print(f"Loaded input dataset with {len(input_df.columns)} columns and {len(input_df)} rows")
    print(f"Loaded output dataset with {len(output_df.columns)} columns and {len(output_df)} rows")
    
    # Print sample data
    print("\nSample input data:")
    print(input_df.head(3))
    print("\nSample output data:")
    print(output_df.head(3))
    
    # 2. Discover rules
    print("Starting rule discovery process...")
    
    # Initialize components
    categorical_handler = CategoricalHandler(categorical_cols)
    feature_engineer = FeatureEngineer(input_df, output_df)
    rule_discoverer = RuleDiscoverer(categorical_handler)
    rule_validator = RuleValidator()
    
    # Check if we need feature engineering (M > N)
    if len(output_df.columns) > len(input_df.columns):
        print(f"M ({len(output_df.columns)}) > N ({len(input_df.columns)}): Performing feature generation...")
        enhanced_input_df = feature_engineer.generate_candidate_features()
    else:
        enhanced_input_df = input_df.copy()
    
    # Segment data by categorical features
    segments = categorical_handler.segment_data(enhanced_input_df)
    print(f"Data segmented into {len(segments)} segments based on categorical features")
    
    # Discover rules per segment
    all_rules = {}
    for segment_name, segment_df in segments.items():
        segment_output = categorical_handler.get_corresponding_output(output_df, segment_df)
        
        print(f"Discovering rules for segment: {segment_name}")
        segment_rules = rule_discoverer.discover_rules(segment_df, segment_output)
        
        # Store discovered rules with segment information
        for output_col, rule in segment_rules.items():
            if output_col not in all_rules:
                all_rules[output_col] = {}
            all_rules[output_col][segment_name] = rule
    
    # 3. Validate rules
    print("\nValidating discovered rules...")
    validation_accuracy = rule_validator.validate_rules(all_rules, enhanced_input_df, output_df, categorical_handler)
    
    print(f"\nOverall rule validation accuracy: {validation_accuracy:.2f}")
    
    # 4. Display discovered rules
    print("\n=== DISCOVERED TRANSFORMATION RULES ===")
    for output_col, rules_by_segment in all_rules.items():
        print(f"\n{output_col}:")
        if len(rules_by_segment) == 1:
            # Single rule for all data
            print(f"  {next(iter(rules_by_segment.values()))}")
        else:
            # Conditional rules
            for segment, rule in rules_by_segment.items():
                print(f"  When {segment}: {rule}")
    print("======================================\n")
    
    # 5. Export discovered rules to JSON
    with open('discovered_rules_real_data.json', 'w') as f:
        json.dump(all_rules, f, indent=2)
    
    print("Rules exported to 'discovered_rules_real_data.json'")

if __name__ == "__main__":
    # Run with synthetic data
    run_example()
    
    # Uncomment to run with real data
    # run_real_data_example(
    #     input_file="input_data.csv",
    #     output_file="output_data.csv",
    #     categorical_cols=["category1", "category2"]
    # )





