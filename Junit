import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import RFE
from sklearn.base import BaseEstimator, TransformerMixin

class NullColumnDropper(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X.dropna(axis=1)

class DynamicFeatureSelector:
    def __init__(self, n_outputs):
        self.n_outputs = n_outputs
        self.models = [RandomForestClassifier(n_estimators=100, random_state=42) for _ in range(n_outputs)]
        self.feature_selectors = [RFE(estimator=RandomForestClassifier(n_estimators=10, random_state=42), n_features_to_select=1, step=1) for _ in range(n_outputs)]
        self.null_droppers = [NullColumnDropper() for _ in range(n_outputs)]
        self.selected_features = [None] * n_outputs

    def fit(self, X, y):
        for i in range(self.n_outputs):
            # Drop columns with null values for this specific record
            X_valid = self.null_droppers[i].fit_transform(X)
            
            # Perform feature selection
            self.feature_selectors[i].fit(X_valid, y.iloc[:, i])
            self.selected_features[i] = X_valid.columns[self.feature_selectors[i].support_].tolist()
            
            # Train the model on selected features
            self.models[i].fit(X_valid[self.selected_features[i]], y.iloc[:, i])

    def predict(self, X):
        predictions = []
        for i in range(self.n_outputs):
            # Drop columns with null values for this specific record
            X_valid = self.null_droppers[i].transform(X)
            
            # Predict using only the selected features
            pred = self.models[i].predict(X_valid[self.selected_features[i]])
            predictions.append(pred)
        
        return np.column_stack(predictions)

def preprocess_data(data):
    # Replace empty strings with NaN
    data = data.replace('', np.nan)
    
    # Encode categorical variables
    categorical_columns = data.select_dtypes(include=['object']).columns
    data = pd.get_dummies(data, columns=categorical_columns)
    
    return data

# Main execution
if __name__ == "__main__":
    # Load your data
    data = pd.read_csv("your_data.csv")
    
    # Preprocess the data
    processed_data = preprocess_data(data)
    
    # Separate features and targets
    X = processed_data.drop(['output1', 'output2', ..., 'outputN'], axis=1)
    y = processed_data[['output1', 'output2', ..., 'outputN']]
    
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Initialize and train the model
    model = DynamicFeatureSelector(n_outputs=y.shape[1])
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Evaluate the model
    for i in range(y.shape[1]):
        accuracy = accuracy_score(y_test.iloc[:, i], y_pred[:, i])
        print(f"Accuracy for output {i+1}: {accuracy}")
        print(f"Selected features for output {i+1}: {model.selected_features[i]}")



id,feature1,feature2,feature3,feature4,feature5,categorical1,categorical2,output
1,10.5,20,30,,50,red,small,A
2,15.2,25,35,45,,blue,medium,B
3,,22,32,42,52,green,large,C
4,12.8,,,40,55,red,medium,A
5,18.3,28,38,48,58,blue,small,B
6,14.7,24,34,,54,green,large,D
7,11.9,21,31,41,,red,medium,C
8,,26,36,46,56,blue,small,A
9,13.6,23,,43,53,green,large,B
10,16.4,27,37,47,57,red,medium,D



class DynamicFeatureSelector:
    def __init__(self):
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.feature_selector = RFE(estimator=RandomForestClassifier(n_estimators=10, random_state=42), n_features_to_select=1, step=1)
        self.null_dropper = NullColumnDropper()
        self.selected_features = None

    def fit(self, X, y):
        # Drop columns with null values for this specific record
        X_valid = self.null_dropper.fit_transform(X)
        
        # Perform feature selection
        self.feature_selector.fit(X_valid, y)
        self.selected_features = X_valid.columns[self.feature_selector.support_].tolist()
        
        # Train the model on selected features
        self.model.fit(X_valid[self.selected_features], y)

    def predict(self, X):
        # Drop columns with null values for this specific record
        X_valid = self.null_dropper.transform(X)
        
        # Predict using only the selected features
        return self.model.predict(X_valid[self.selected_features])

# Main execution
if __name__ == "__main__":
    # Load your data
    data = pd.read_csv("sample_data.csv")
    
    # Preprocess the data
    processed_data = preprocess_data(data)
    
    # Separate features and targets
    X = processed_data.drop(['id', 'output'], axis=1)
    y = processed_data['output']
    
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Initialize and train the model
    model = DynamicFeatureSelector()
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy}")
    print(f"Selected features: {model.selected_features}")



import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import RFE
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import LabelEncoder

class NullColumnDropper(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X.dropna(axis=1)

class DynamicFeatureSelector:
    def __init__(self):
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.feature_selector = RFE(estimator=RandomForestClassifier(n_estimators=10, random_state=42), n_features_to_select=1, step=1)
        self.null_dropper = NullColumnDropper()
        self.selected_features = None
        self.label_encoder = LabelEncoder()

    def fit(self, X, y):
        # Encode the target variable
        y_encoded = self.label_encoder.fit_transform(y)
        
        # Drop columns with null values for this specific record
        X_valid = self.null_dropper.fit_transform(X)
        
        # Perform feature selection
        self.feature_selector.fit(X_valid, y_encoded)
        self.selected_features = X_valid.columns[self.feature_selector.support_].tolist()
        
        # Train the model on selected features
        self.model.fit(X_valid[self.selected_features], y_encoded)

    def predict(self, X):
        # Drop columns with null values for this specific record
        X_valid = self.null_dropper.transform(X)
        
        # Predict using only the selected features
        predictions_encoded = self.model.predict(X_valid[self.selected_features])
        
        # Decode the predictions back to original labels
        return self.label_encoder.inverse_transform(predictions_encoded)

def preprocess_data(data):
    # Create a copy of the data to avoid modifying the original
    df = data.copy()
    
    # Replace empty strings with NaN
    df = df.replace('', np.nan)
    
    # Identify numeric and categorical columns
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    categorical_columns = df.select_dtypes(exclude=[np.number]).columns.drop('output')
    
    # Handle categorical variables
    for col in categorical_columns:
        # Create dummy variables for each categorical column
        dummies = pd.get_dummies(df[col], prefix=col, dummy_na=True)
        # Add the dummy variables to the dataframe
        df = pd.concat([df, dummies], axis=1)
        # Drop the original categorical column
        df = df.drop(col, axis=1)
    
    # Convert the 'output' column to categorical type
    df['output'] = df['output'].astype('category')
    
    return df

# Main execution
if __name__ == "__main__":
    # Load your data
    data = pd.read_csv("sample_data.csv")
    
    # Preprocess the data
    processed_data = preprocess_data(data)
    
    # Separate features and targets
    X = processed_data.drop(['id', 'output'], axis=1)
    y = processed_data['output']
    
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Initialize and train the model
    model = DynamicFeatureSelector()
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)




import pandas as pd
import numpy as np
from sklearn.datasets import make_classification

def generate_large_dataset(n_samples=10000, n_features=10, n_categorical=3, n_classes=4):
    # Generate a synthetic dataset
    X, y = make_classification(n_samples=n_samples, 
                               n_features=n_features-n_categorical, 
                               n_classes=n_classes, 
                               n_informative=5, 
                               n_redundant=2, 
                               n_repeated=0, 
                               n_clusters_per_class=2,
                               random_state=42)
    
    # Create a DataFrame
    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features-n_categorical)])
    
    # Add categorical features
    for i in range(n_categorical):
        df[f'categorical_{i}'] = np.random.choice(['A', 'B', 'C', 'D', 'E'], size=n_samples)
    
    # Add some null values
    for col in df.columns:
        mask = np.random.random(n_samples) < 0.05  # 5% of values will be null
        df.loc[mask, col] = np.nan
    
    # Add the output column
    df['output'] = [chr(65 + i) for i in y]  # Convert to letters A, B, C, D
    
    # Add an ID column
    df.insert(0, 'id', range(1, n_samples + 1))
    
    return df

# Generate the dataset
large_df = generate_large_dataset()

# Save to CSV
large_df.to_csv('large_training_dataset.csv', index=False)

print("Large dataset has been generated and saved to 'large_training_dataset.csv'")
print(f"Dataset shape: {large_df.shape}")
print("\nSample of the dataset:")
print(large_df.head())
print("\nDataset info:")
print(large_df.info())
    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy}")
    print(f"Selected features: {model.selected_features}")
