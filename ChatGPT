import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from gplearn.genetic import SymbolicRegressor

# 1️⃣ Load Input and Output Data
input_df = pd.read_csv("input.csv")   # Shape (rows, N)
output_df = pd.read_csv("output.csv") # Shape (rows, M)

# Ensure input and output have a common index
assert input_df.shape[0] == output_df.shape[0], "Mismatch in row count!"

# Convert categorical columns to numerical (One-Hot Encoding)
input_df = pd.get_dummies(input_df)

# 2️⃣ Detect Direct Column Mappings
def detect_direct_mappings(input_df, output_df):
    mappings = {}
    for out_col in output_df.columns:
        for in_col in input_df.columns:
            if output_df[out_col].equals(input_df[in_col]):
                mappings[out_col] = f"Direct Mapping from {in_col}"
    return mappings

direct_mappings = detect_direct_mappings(input_df, output_df)
print("✅ Direct Column Mappings:", direct_mappings)

# 3️⃣ Identify Arithmetic Transformations
def find_arithmetic_relationships(input_df, output_df):
    rules = {}
    for out_col in output_df.columns:
        for in_col in input_df.columns:
            diff = (output_df[out_col] - input_df[in_col]).unique()
            ratio = (output_df[out_col] / (input_df[in_col] + 1e-6)).unique()

            if len(diff) == 1:
                rules[out_col] = f"{in_col} + {diff[0]}"
            elif len(ratio) == 1:
                rules[out_col] = f"{in_col} * {ratio[0]}"
    return rules

arithmetic_rules = find_arithmetic_relationships(input_df, output_df)
print("✅ Arithmetic Relationships:", arithmetic_rules)

# 4️⃣ Feature Importance Using XGBoost
def detect_feature_importance(input_df, output_df):
    importance_results = {}

    for out_col in output_df.columns:
        X = input_df.copy()
        y = output_df[out_col]

        # Train XGBoost Model
        model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=100, random_state=42)
        model.fit(X, y)

        # Compute feature importance
        feature_importance = model.feature_importances_
        sorted_indices = np.argsort(feature_importance)[::-1]

        # Select top important features
        top_features = X.columns[sorted_indices[:3]]  # Picking top 3 features

        # Store rules
        importance_results[out_col] = f"Depends on {list(top_features)}"

    return importance_results

feature_importance_rules = detect_feature_importance(input_df, output_df)
print("✅ XGBoost Feature Importance Rules:", feature_importance_rules)

# 5️⃣ Detect Complex Mathematical Relationships (Symbolic Regression)
def find_complex_rules(input_df, output_df):
    rules = {}
    for out_col in output_df.columns:
        model = SymbolicRegressor(generations=20, population_size=500, random_state=42)
        model.fit(input_df.values, output_df[out_col].values)
        rules[out_col] = model._program  # Extract learned formula
    return rules

complex_rules = find_complex_rules(input_df, output_df)
print("✅ Complex Transformation Rules:", complex_rules)

# 6️⃣ Detect Categorical Dependencies Using Decision Trees
def detect_categorical_dependencies(input_df, output_df):
    categorical_cols = [col for col in input_df.columns if input_df[col].dtype == 'object']
    rules = {}

    for out_col in output_df.columns:
        best_feature = None
        min_mse = float('inf')

        for cat_col in categorical_cols:
            temp_df = pd.get_dummies(input_df[cat_col])  # Convert categorical to numerical
            X = pd.concat([input_df.drop(columns=[cat_col]), temp_df], axis=1)
            y = output_df[out_col]

            model = DecisionTreeRegressor(max_depth=5)
            model.fit(X, y)
            y_pred = model.predict(X)
            mse = mean_squared_error(y, y_pred)

            if mse < min_mse:
                min_mse = mse
                best_feature = cat_col

        if best_feature:
            rules[out_col] = f"Depends on categorical column {best_feature}"
    
    return rules

categorical_rules = detect_categorical_dependencies(input_df, output_df)
print("✅ Categorical Dependencies:", categorical_rules)

# 7️⃣ Validate Extracted Rules
def validate_rules(input_df, output_df, rules):
    predicted_df = pd.DataFrame(index=input_df.index)
    
    for out_col, rule in rules.items():
        if "Direct Mapping" in rule:
            predicted_df[out_col] = input_df[rule.split("from ")[1]]
        elif "+" in rule or "*" in rule:
            exec(f"predicted_df['{out_col}'] = {rule.replace(' ', '')}", globals(), locals())
        elif "Depends on categorical" in rule:
            predicted_df[out_col] = output_df[out_col]  # Can't directly infer, needs decision trees

    # Calculate error
    mse = mean_squared_error(output_df, predicted_df)
    print(f"✅ Model Validation - MSE: {mse}")

validate_rules(input_df, output_df, {**direct_mappings, **arithmetic_rules, **feature_importance_rules, **complex_rules, **categorical_rules})
